# AeroScrap Backend - Project Overview

**High-Level Architecture and System Documentation**

---

## ğŸ“‹ Table of Contents

1. [System Architecture](#system-architecture)
2. [Core Components](#core-components)
3. [Data Flow](#data-flow)
4. [Database Schema](#database-schema)
5. [API Architecture](#api-architecture)
6. [Scraper System](#scraper-system)
7. [Integration Points](#integration-points)
8. [Security Model](#security-model)
9. [Deployment Strategy](#deployment-strategy)
10. [Future Enhancements](#future-enhancements)

---

## ğŸ—ï¸ System Architecture

### Overview
AeroScrap Backend is a Django-based monolithic application with modular components for job scraping, resume analysis, and data management. The system follows a traditional Django app structure with clear separation of concerns.

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Client Layer                            â”‚
â”‚  (Web Browser, Mobile App, External APIs, Cron Jobs)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ HTTP/HTTPS
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API Gateway Layer                         â”‚
â”‚           Django Ninja REST API (/api/)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Jobs API    â”‚  â”‚ Resumes API  â”‚  â”‚  Admin API   â”‚      â”‚
â”‚  â”‚  /api/jobs/  â”‚  â”‚   /api/      â”‚  â”‚  /admin/     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Application Layer                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Jobs App     â”‚  â”‚  Resumes App   â”‚  â”‚  Auth System  â”‚ â”‚
â”‚  â”‚                â”‚  â”‚                â”‚  â”‚               â”‚ â”‚
â”‚  â”‚ â€¢ Models       â”‚  â”‚ â€¢ Models       â”‚  â”‚ â€¢ API Keys    â”‚ â”‚
â”‚  â”‚ â€¢ Dedup Logic  â”‚  â”‚ â€¢ Parser       â”‚  â”‚ â€¢ Bearer Auth â”‚ â”‚
â”‚  â”‚ â€¢ Classificationâ”‚ â”‚ â€¢ Extraction   â”‚  â”‚               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Layer                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Django ORM (Object-Relational Mapping)               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                   â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Database (SQLite / PostgreSQL)                       â”‚ â”‚
â”‚  â”‚  â€¢ jobs_job             â€¢ resumes_resume              â”‚ â”‚
â”‚  â”‚  â€¢ jobs_companymapping  â€¢ jobs_crawllog               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 External Scraper Layer                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Scraper Orchestrator (daily_scraper_to_db.py)      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚             â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Aviation Scraper  â”‚ Air India   â”‚ Goose Scraper    â”‚    â”‚
â”‚  â”‚ (BeautifulSoup)   â”‚ (Requests)  â”‚ (Playwright)     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚             â”‚                                â”‚               â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â–º Direct DB Insert â—„â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Automation Layer                           â”‚
â”‚  â€¢ Cron Jobs (Linux)                                         â”‚
â”‚  â€¢ Systemd Timers (Linux)                                    â”‚
â”‚  â€¢ Python Schedule (Cross-platform)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Core Components

### 1. **Jobs App** (`jobs/`)
**Purpose**: Manage aviation job listings from multiple sources

**Key Files**:
- `models.py` - Job, CompanyMapping, CrawlLog models
- `api.py` - REST endpoints for job operations
- `utils.py` - Deduplication and classification logic
- `auth.py` - API authentication

**Responsibilities**:
- Store and manage job listings
- Handle deduplication (URL-based + fuzzy matching)
- Classify companies by operation type
- Track scraping history via CrawlLog
- Export data (CSV, JSON)

### 2. **Resumes App** (`resumes/`)
**Purpose**: Parse and analyze aviation resumes

**Key Files**:
- `models.py` - Resume model
- `api.py` - Resume upload and retrieval endpoints
- `resume_parser.py` - Text extraction and parsing
- `utils.py` - JSON backup helpers

**Responsibilities**:
- Accept resume uploads (PDF, DOCX, TXT)
- Extract structured data (name, email, experience, skills)
- Store parsed data in database
- Maintain JSON backup (ResumeDataStore.json)

### 3. **Scrapers** (`scrapers/`)
**Purpose**: Automated data collection from external sources

**Key Files**:
- `daily_scraper_to_db.py` - Main orchestrator
- `aviationjobsearchScrap.py` - Aviation job board scraper
- `airIndiaScrap.py` - Air India careers scraper
- `gooseScrap.py` - Goose Recruitment scraper
- `linkdinScraperRT.py` - LinkedIn job scraper

**Responsibilities**:
- Scrape job listings from target websites
- Normalize data into standard format
- Direct database insertion
- Handle errors and retries
- Log all activities

### 4. **Backend Main** (`backendMain/`)
**Purpose**: Django project configuration

**Key Files**:
- `settings.py` - Django configuration, database, middleware
- `urls.py` - URL routing and API mounting
- `wsgi.py` / `asgi.py` - Server gateways

---

## ğŸ”„ Data Flow

### 1. **Job Scraping Flow**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Scheduler  â”‚ (Cron/Systemd/Schedule)
â”‚  Triggers    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  daily_scraper_to_db.py         â”‚
â”‚  â€¢ Initialize Django            â”‚
â”‚  â€¢ Setup logging                â”‚
â”‚  â€¢ Run all scrapers in sequence â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â–º Aviation Scraper â”€â”€â”€â”€â–º Returns 75 jobs (dict format)
       â”‚
       â”œâ”€â”€â”€â”€â–º Air India Scraper â”€â”€â–º Returns 28 jobs (dict format)
       â”‚
       â”œâ”€â”€â”€â”€â–º Goose Scraper â”€â”€â”€â”€â”€â”€â”€â–º Returns 12 jobs (dict format)
       â”‚
       â””â”€â”€â”€â”€â–º LinkedIn Scraper â”€â”€â”€â”€â–º Returns N jobs (dict format)
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Normalize Job Data             â”‚
â”‚  â€¢ Convert to standard format   â”‚
â”‚  â€¢ Fill missing fields          â”‚
â”‚  â€¢ Validate data types          â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  insert_or_update_job()         â”‚
â”‚  â€¢ Check URL duplication        â”‚
â”‚  â€¢ Check fuzzy title match      â”‚
â”‚  â€¢ Insert new or update existingâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Django ORM                     â”‚
â”‚  â€¢ Transaction per job          â”‚
â”‚  â€¢ Save to database             â”‚
â”‚  â€¢ Log result                   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Database (jobs_job table)      â”‚
â”‚  â€¢ Job record created/updated   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. **API Request Flow**

```
Client Request
     â”‚
     â–¼
Django Middleware Stack
     â”‚
     â”œâ”€â”€â–º CORS Check
     â”œâ”€â”€â–º CSRF Validation (if applicable)
     â”œâ”€â”€â–º Authentication (Bearer token)
     â”‚
     â–¼
Django Ninja Router
     â”‚
     â”œâ”€â”€â–º Route matching (/api/jobs/...)
     â”œâ”€â”€â–º Parameter validation
     â”‚
     â–¼
API Endpoint Handler (jobs/api.py)
     â”‚
     â”œâ”€â”€â–º Business logic execution
     â”œâ”€â”€â–º Database queries (ORM)
     â”œâ”€â”€â–º Data transformation
     â”‚
     â–¼
Response Serialization
     â”‚
     â–¼
JSON Response to Client
```

### 3. **Resume Processing Flow**

```
Resume Upload (POST /api/upload)
     â”‚
     â–¼
File Validation
     â”‚
     â”œâ”€â”€â–º Check file type (PDF, DOCX, TXT)
     â”œâ”€â”€â–º Check file size
     â”‚
     â–¼
resume_parser.py
     â”‚
     â”œâ”€â”€â–º Extract text content
     â”œâ”€â”€â–º Parse sections (education, experience, skills)
     â”œâ”€â”€â–º Extract contact info
     â”‚
     â–¼
Create Resume Object
     â”‚
     â”œâ”€â”€â–º Save to database (resumes_resume)
     â”œâ”€â”€â–º Backup to ResumeDataStore.json
     â”‚
     â–¼
Return Parsed Data
```

---

## ğŸ—„ï¸ Database Schema

### **jobs_job** (Main Job Listings Table)

| Column | Type | Description |
|--------|------|-------------|
| id | BigAutoField | Primary key |
| title | CharField(500) | Job title |
| normalized_title | CharField(500) | Normalized job title |
| company | CharField(300) | Company name |
| company_id | CharField(100) | External company ID |
| country_code | CharField(2) | ISO country code |
| operation_type | CharField(50) | Airline/MRO/Airport/etc. |
| posted_date | DateField | When job was posted |
| url | URLField(1000) | Job listing URL (unique) |
| description | TextField | Full job description |
| status | CharField(20) | active/expired/filled |
| senior_flag | BooleanField | Is senior position |
| source | CharField(100) | Scraper source |
| last_checked | DateTimeField | Last verification |
| raw_json | JSONField | Original scraped data |
| created_at | DateTimeField | Record creation |
| updated_at | DateTimeField | Last update |

**Indexes**: url (unique), company, country_code, operation_type, posted_date

### **jobs_companymapping** (Company Classification)

| Column | Type | Description |
|--------|------|-------------|
| id | BigAutoField | Primary key |
| company_name | CharField(300) | Display name |
| normalized_name | CharField(300) | Lowercase normalized (unique) |
| operation_type | CharField(50) | Company type |
| country_code | CharField(2) | Primary country |
| notes | TextField | Additional info |
| created_at | DateTimeField | Record creation |
| updated_at | DateTimeField | Last update |

### **jobs_crawllog** (Scraping History)

| Column | Type | Description |
|--------|------|-------------|
| id | BigAutoField | Primary key |
| source | CharField(100) | Scraper name |
| run_time | DateTimeField | Execution timestamp |
| items_found | IntegerField | Total items scraped |
| items_inserted | IntegerField | New records |
| items_updated | IntegerField | Updated records |
| error | TextField | Error messages |

### **resumes_resume** (Resume Data)

| Column | Type | Description |
|--------|------|-------------|
| id | BigAutoField | Primary key |
| name | CharField(200) | Candidate name |
| email | EmailField | Contact email |
| phone | CharField(20) | Phone number |
| raw_text | TextField | Full resume text |
| parsed_data | JSONField | Structured data |
| file_path | CharField(500) | Original file path |
| created_at | DateTimeField | Upload time |
| updated_at | DateTimeField | Last update |

---

## ğŸ”Œ API Architecture

### **Django Ninja Framework**
- Schema-based validation
- Auto-generated OpenAPI docs
- Type hints for endpoints
- Built-in authentication

### **API Statistics**

- **Total Endpoints**: 31+ REST API endpoints
- **Public Endpoints**: 24 (no authentication required)
- **Protected Endpoints**: 7 (require API key)
- **Categories**: 9 major categories
  - Core Job Operations (7 endpoints)
  - Advanced Search (1 endpoint)
  - Company APIs (4 endpoints)
  - Analytics (4 endpoints)
  - Recent Activity (3 endpoints)
  - Job Comparison (2 endpoints)
  - Export & Reports (2 endpoints)
  - Scraper Management (2 endpoints)
  - Admin Operations (6 endpoints)

### **Endpoint Organization**

```
/api/
â”œâ”€â”€ jobs/                           # Job management (31+ endpoints)
â”‚   â”œâ”€â”€ /                          # List jobs (GET)
â”‚   â”œâ”€â”€ /id/{id}                   # Single job (GET)
â”‚   â”œâ”€â”€ /ingest                    # Single ingest (POST, auth)
â”‚   â”œâ”€â”€ /bulk_ingest               # Bulk ingest (POST, auth)
â”‚   â”œâ”€â”€ /search                    # Simple search (GET)
â”‚   â”œâ”€â”€ /advanced-search           # âœ¨ Advanced multi-filter search (GET)
â”‚   â”œâ”€â”€ /stats                     # Statistics (GET)
â”‚   â”œâ”€â”€ /health                    # Health check (GET)
â”‚   â”‚
â”‚   â”œâ”€â”€ /companies/                # âœ¨ Company APIs
â”‚   â”‚   â”œâ”€â”€ /                     # List all companies (GET)
â”‚   â”‚   â”œâ”€â”€ /{name}               # Company profile (GET)
â”‚   â”‚   â”œâ”€â”€ /{name}/jobs          # Company's jobs (GET)
â”‚   â”‚   â””â”€â”€ /trending             # Trending companies (GET)
â”‚   â”‚
â”‚   â”œâ”€â”€ /analytics/                # âœ¨ Analytics & Insights
â”‚   â”‚   â”œâ”€â”€ /trends               # Job posting trends (GET)
â”‚   â”‚   â”œâ”€â”€ /geographic           # Geographic distribution (GET)
â”‚   â”‚   â””â”€â”€ /operation-types      # Stats by type (GET)
â”‚   â”‚
â”‚   â”œâ”€â”€ /recent                    # âœ¨ Recently added jobs (GET)
â”‚   â”œâ”€â”€ /updated                   # âœ¨ Recently updated jobs (GET)
â”‚   â”œâ”€â”€ /alerts/senior             # Senior role alerts (GET)
â”‚   â”‚
â”‚   â”œâ”€â”€ /compare                   # âœ¨ Compare multiple jobs (POST)
â”‚   â”œâ”€â”€ /similar/{id}              # âœ¨ Find similar jobs (GET)
â”‚   â”‚
â”‚   â”œâ”€â”€ /export/
â”‚   â”‚   â”œâ”€â”€ /daily.csv            # Daily CSV export (GET)
â”‚   â”‚   â””â”€â”€ /json                 # âœ¨ JSON export with filters (GET)
â”‚   â”‚
â”‚   â””â”€â”€ /admin/                    # Admin operations (auth)
â”‚       â”œâ”€â”€ /company-mappings     # Manage mappings (CRUD)
â”‚       â”œâ”€â”€ /unknown-companies    # Companies without mapping (GET)
â”‚       â”œâ”€â”€ /scrapers/status      # âœ¨ Scraper status (GET, auth)
â”‚       â””â”€â”€ /scrapers/logs        # âœ¨ Scraper logs (GET, auth)
â”‚
â”œâ”€â”€ upload-resume                   # Resume upload (POST)
â”œâ”€â”€ upload-resume-with-info         # Resume with metadata (POST)
â”œâ”€â”€ resumes                         # List resumes (GET)
â”œâ”€â”€ resumes/{id}                    # Single resume (GET)
â”œâ”€â”€ resumes/{id}/download           # Download resume file (GET)
â””â”€â”€ stats                           # Resume statistics (GET)
```

âœ¨ **New in v2.0** - 15+ new endpoints added

### **Authentication**
- **Type**: Bearer Token (HTTP Authorization header)
- **Implementation**: `jobs/auth.py` - APIKeyAuth class
- **Configuration**: `ADMIN_API_KEY` environment variable
- **Scope**: Protected endpoints (ingest, bulk_ingest, PATCH, DELETE)

---

## ğŸ•·ï¸ Scraper System

### **Scraper Technologies**

| Scraper | Technology | Method | Output |
|---------|------------|--------|--------|
| Aviation Jobs | BeautifulSoup4 + Requests | HTML parsing | 75 jobs/run |
| Air India | Requests + JSON | API endpoint | 28 jobs/run |
| Goose | Playwright | Headless browser | 12 jobs/run |
| LinkedIn | Playwright (manual) | Browser automation | Variable |

### **Normalization Pipeline**

Each scraper returns different formats, normalized to:

```python
{
    'title': str,
    'company': str,
    'url': str,
    'description': str,
    'posted_date': date or None,
    'country_code': str,
    'operation_type': str or None,
    'source': str
}
```

### **Deduplication Strategy**

**Primary**: URL matching
- If URL exists â†’ update existing record

**Secondary**: Fuzzy title + company matching
- Levenshtein distance < 85% similarity
- Same company name
- Same posted date
- â†’ Treat as duplicate, update existing

### **Error Handling**

- Individual transaction per job (prevents cascade failure)
- NULL field fallbacks (company â†’ 'Unknown Company')
- Network timeout handling (job skipped, logged)
- Comprehensive logging to `scrapers/logs/`

---

## ğŸ”— Integration Points

### **External Services**
Currently self-contained, ready for:
- Email notifications (SMTP)
- Sentry error tracking
- Redis caching
- Elasticsearch for search

### **Internal Integration**
- Scrapers â†’ Direct database insert (bypasses API for performance)
- API â†’ Can be consumed by frontend, mobile, external systems
- Admin panel â†’ Django built-in admin for manual management

---

## ğŸ”’ Security Model

### **Authentication**
- API key-based authentication
- Environment variable configuration
- Optional (disabled if `ADMIN_API_KEY` not set)

### **CORS**
- Currently allows all origins (development)
- Should be restricted in production

### **Database**
- ORM prevents SQL injection
- Prepared statements
- Input validation via Django forms/serializers

### **Secret Management**
- `.env` file for sensitive data
- `.env.example` template
- `.gitignore` prevents commits

### **Production Recommendations**
- Use PostgreSQL (not SQLite)
- Enable HTTPS only
- Restrict `ALLOWED_HOSTS`
- Rotate `SECRET_KEY`
- Set `DEBUG=0`
- Use secrets manager (AWS Secrets Manager, Vault)

---

## ğŸš€ Deployment Strategy

### **Development**
```bash
python manage.py runserver  # SQLite, DEBUG=1
```

### **Staging/Production**

**Option 1: Traditional Server**
```bash
# Install dependencies
pip install gunicorn
pip install -r requirements.txt

# Run with Gunicorn
gunicorn backendMain.wsgi:application \
  --bind 0.0.0.0:8000 \
  --workers 4 \
  --timeout 120

# Nginx reverse proxy
# PostgreSQL database
# Systemd service for scrapers
```

**Option 2: Docker**
```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["gunicorn", "backendMain.wsgi:application"]
```

**Option 3: Platform-as-a-Service**
- Heroku
- Railway
- Render
- AWS Elastic Beanstalk

### **Scraper Scheduling**

**Development**: Manual runs
**Production**: 
- Cron job (Linux servers)
- Systemd timer (Linux servers)
- AWS EventBridge + Lambda
- Kubernetes CronJob

---

## ğŸ”® Future Enhancements

### **Short-term**
- [ ] Email notifications for scraper failures
- [ ] Prometheus metrics endpoint
- [ ] API rate limiting
- [ ] Frontend dashboard (React/Vue)

### **Medium-term**
- [ ] Elasticsearch integration for full-text search
- [ ] Redis caching for frequently accessed data
- [ ] WebSocket support for real-time job alerts
- [ ] ML-based job recommendation engine

### **Long-term**
- [ ] Microservices architecture (scraper service separate)
- [ ] GraphQL API alongside REST
- [ ] Mobile app (React Native)
- [ ] AI-powered resume matching
- [ ] Multi-tenant support for different aviation sectors

---

## ğŸ“Š Performance Metrics

### **Current Benchmarks**
- Job scraping: ~115 jobs in 10 minutes
- API response time: < 100ms (average)
- Database size: ~1MB per 1000 jobs
- Deduplication accuracy: 84% insert rate

### **Scalability**
- SQLite: Up to 10K jobs (development)
- PostgreSQL: Millions of jobs (production)
- API throughput: 100+ req/sec (with Gunicorn)

---

## ğŸ“ Support & Maintenance

### **Logs Location**
- Django: Console output
- Scrapers: `scrapers/logs/daily_scraper_YYYYMMDD.log`
- Django errors: `backendMain/django_errors.log` (if configured)

### **Health Checks**
- API: `GET /api/jobs/health`
- Database: `python manage.py check --database default`

### **Backup Strategy**
- Database: Daily PostgreSQL dumps
- Resumes: File system backup + JSON backup
- Code: Git repository

---

**Document Version**: 1.0  
**Last Updated**: November 2025  
**Maintainer**: AeroOps Intel Development Team
